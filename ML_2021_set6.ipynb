{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions to Modern Data Science Problems (probelms till 15.01)\n",
    "*by Mateusz Kmieć* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#standard imports \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 35\n",
    "We consider two models for classification: (1) “flexible” (e.g. kNN) (2) “rigid ” (e.g. linear). Discuss which would you typically choose in those situations:\n",
    "* A small number of training samples N\n",
    "\n",
    "*Rigid* \n",
    "\n",
    "* A large number of features\n",
    "\n",
    "*Rigid* \n",
    "\n",
    "* A large number of training samples N\n",
    "\n",
    "*Flexible*\n",
    "\n",
    "* Highly non-linear behavior\n",
    "\n",
    "\n",
    "*Flexible*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 36.\n",
    "\n",
    "Add feature scaling (standardization and normalization) for your k-NN implementation.\n",
    "Check if it affects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 37\n",
    "We want to check if the coin is fair, we obtained the following result: X = {H,T,H,H,T,T,H,H,H}.\n",
    "* define the PMF\n",
    "* define the likelihood function\n",
    "* calculate the maximum value estimator of frequency of heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 38\n",
    "When MAP and MLE give the same results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 39\n",
    "We want to check if the coin is fair, we estimate the frequency of heads. Calculate the MAP maximum value estimator, when the prior distribution is described by the beta function\n",
    "$$\\text{pdf}(p|\\alpha, \\beta) = (\\alpha-1)! * ( \\beta-1)!/(\\alpha+ \\beta -1)! * p\\alpha-1 * (1-p)\\beta-1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 40\n",
    "Consider the case of $\\lambda >> 1$, and $\\lambda \\approx 0$: how does it affect the overfitting/bias problem? Will the procedure work correctly anyway?\n",
    "\n",
    "$$ E_T[L(f_\\theta(x),Y)]=\\frac{1}{2N} \\sum (\\theta^T x^{(i)}-y^{(i)})^2 + \\frac{\\lambda}{2}\\sum \\theta_j^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 41\n",
    "Read the code and try to understand how it works. By changing the values of degree and alpha observe the interplay of overfitting/bias\n",
    "* for which values one can obtained the reasonable fit to data ?\n",
    "* plot the training and test errors as a function of polynomial degree (for fixed alpha)\n",
    "* plot the training and test errors as a function of regularization term (for fixed degree). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 42\n",
    "We consider a linear regression model. Let’s assume that the error model is additive $y= f(x) + \\epsilon$ , and errors are have Gaussian distribution with 0 mean ($E[\\epsilon] =0$) and variance\n",
    "($\\text{Var}[\\epsilon] =\\sigma^2$). We In addition, assume that our parameters are described by:\n",
    "* Gaussian Distribution, with 0 mean, and some variance\n",
    "* Laplace Distribution, with 0 mean, and some variance\n",
    "Derive the minimization problem starting from the MAP approach, taking into account the prior distribution of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem  43 \n",
    "We consider a classification model with some regularization scheme. Which statements\n",
    "are true? Justify your choice.\n",
    "* Adding regularization would typically lower the training error\n",
    " \n",
    "FALSE \n",
    "\n",
    "* Adding new features to your model would make the training error higher or the same\n",
    "\n",
    "FLASE/TRUE\n",
    "\n",
    "* Adding regularization would typically lower the test error\n",
    "\n",
    "TRUE\n",
    "\n",
    "* Adding regularization can cause overfitting of your model\n",
    "\n",
    "FALSE\n",
    "\n",
    "* Using a large value of the lambda parameter can cause the overfitting of your model\n",
    "\n",
    "FALSE\n",
    "\n",
    "\n",
    "* Adding regularization can introduce the bias to your model\n",
    "\n",
    "TRUE\n",
    "\n",
    "* Adding regularization can make the training error higher\n",
    "\n",
    "\n",
    "TRUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 44\n",
    "Calculate the derivative of the sigmoid function and express it using the sigmoid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.ai.mit.edu/courses/6.892/lecture8-html/sld015.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 45 \n",
    "Assuming the loss function definition as:\n",
    "$$ L(f_\\theta(x),Y)= -log f_\\theta(x) \\text{ for $Y=1$}\\\\\n",
    "                   = -log (1-f_\\theta(x)) \\text{ for $Y=0$}$$\n",
    "what are the loss values for:\n",
    "* $f_\\theta(x)=0$, Y=0\n",
    "* $f_\\theta(x)=1$, Y=0\n",
    "* $f_\\theta(x)=0$, Y=1\n",
    "* $f_\\theta(x)=1$, Y=1\n",
    "\n",
    "Draw L as a function of $f_\\theta(x)$ for Y= 0 and Y=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 46\n",
    "Calculate the equation of the decision boundary for the logistic regression model assuming:\n",
    "\n",
    "$$ f_\\theta(x)\\geq 0.5 \\to Y=1\\\\\n",
    "f_\\theta(x)< 0.5 \\to Y=0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 47\n",
    "* For which value of K K-fold cross-validation is equivalent to Leave-one-out method?\n",
    "* Order the cross-validation methods (K-fold e.g. K=5, leave-one-out and the simple splitting into validation and training sets) in the ascending cost of the computation power\n",
    "* For which K the K-fold cross-validation requires the most of the computing power?\n",
    "\n",
    "Justify your answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 48\n",
    "We consider two cases of binary classifier tests:\n",
    "* A - airplanes security checks before the take-off (Y=’allowed’/’denied’)\n",
    "* B - decision if according to her/his profile a client will be potentially interested to buy a new product and if to show her/him the targeted advertisement(Y=’yes’/’no’)\n",
    "\n",
    "How would we decide how to tune our classifier threshold for each case in the context of sensitivity vs specificity trade-off? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
