{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions to Modern Data Science Problems (probelms till 22.01 and 5.02)\n",
    "disclaimer: none optional problems presented here!\n",
    "I dedicated a separated notebook to them.\n",
    "\n",
    "*by Mateusz Kmieć*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 49\n",
    "Discuss the performance of different classification models (A-E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](roc.png)\n",
    "Figure.1:   source: http://koza.if.uj.edu.pl/~krzemien/machine_learning2021/materials/problems_till_lecture_10.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The best performance is for D since TPR=1 and FPR is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. As far as A, B, C and E the usefulness of the chosen classifier is dependent on the classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For A we have very low FPR and fairly high TPR. Such classifier could be useful for detection of crime. We don't want to falsely accuse somebody of crime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. As fas as B is concerned It has quite high TPR and less than satisfactory FPR. It might be useful for COVID detection. We have high TPR at the expense of FPR. In the case of COVID detection we are not as much concerned with high FPR results detection as we are with low TPR. It is better to be careful than sorry. At worst people are classified as ill and have to quarantine for two weeks. In the opposite case we are at risk of causing accelerated spread of the deadly disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. As for C it lies almost on the random guessing line and so we have nearly no predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. On the one hand for E we have a very high FPR and low TPR and so it is the worst classifier, it is even worse than C. On the other hand we could invert the condition of this classifier and obtain a classifier with a decent predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 50\n",
    "We have k = 10 classes and a binary model. How many comparisons do we need to perform using all-vs-one approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to perform 10 comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 53\n",
    "For which method, there is a guarantee that it will converge to the global minimum. Justify your choice if possible:\n",
    "\n",
    "1. GD\n",
    "\n",
    "2. SGD\n",
    "\n",
    "3. ADAM\n",
    "\n",
    "4. Newton’s method\n",
    "\n",
    "5. All of them\n",
    "\n",
    "6. None of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of them. We cannot even guarantee their convergence and when they converge it is a convergence to the local minimum. However we can expect them to converge to the global minimum for purely convex functions.  Newton-method usually converges much faster than GD. Newton-method invlolves inverting hessian. For high dimensions Hessian might be too expensive to \n",
    "invert. Although for large sets of data the best convergence is usually observed for ADAM, the best generalisation power is attributed to SGD-like methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 54\n",
    "We use a large dataset with a multidimensional features space. Which method would be probably appropriate to use? Justify your choice if possible\n",
    "\n",
    "1. GD\n",
    "2. SGD\n",
    "3. ADAM\n",
    "4. Newton’s method\n",
    "5. All of them\n",
    "6. None of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton-method usually converges much faster than GD. Newton-method invlolves inverting Hessian. For high dimensions Hessian might be too expensive to \n",
    "invert. GD might turn out to be too computationally expensive for large datasets that is why for large sets of data we should use SGD or Adam. At some point Adam was recommended over SGD (lower training errors, faster convergence).\n",
    "This recommendation is questioned by several studies (SGD-like methods generalize better in \n",
    "many cases than Adam-like methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 55\n",
    "Which of the statements are true?\n",
    "\n",
    "1. SGD is guaranteed to decrease the cost function in every iteration False (I believe that is the case for GD.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. SGD uses small subsample (or one sample) to update the parameters True (One random point exactly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. When using SGD we don't need to randomize the data sample. False (Well we either randomise the set or pick pints at random. Whatever suits you best, who am I to judege!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. When using GD we don't need to randomize the data sample. True (For SGD it is necessary to randomise the data (or pick samples at random), but in this the case of GD it is not as we use the whole dataset each time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 56\n",
    "Which of the statements are true?\n",
    "\n",
    "1. GD with a very small learning rate can be ineffective in a sense that it would take a lot of time to find the solution (to converge). True (It is self-evident that a small step = a lot of time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. SGD with a very large learning rate can be ineffective in a sense that it would take a lot of time to find the solution (to converge). True (It is difficult make a hole in golf if we do not use less and less force when as we get closer to target.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If we choose small enough learning rate, then we are guaranteed to converge to the same solution irrespective of the choice of the initial values. False (Well we can walk down from the top of a hill using the same step size within 360 degree angle. Depending on the choice of direction at the top we will end up in a different position once we make it down the hill.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. When the value of learning rate is too large then we can \"overshoot\" the solution. True (The argumentation here is very similar to the explanation presented in point 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 57\n",
    "Which of the statements are true?\n",
    "\n",
    "1. Newton's method uses information about the Hessian. True (It uses both gradient and hessian)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. GD uses information about the Hessian. False (It uses only gradient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Newton's method in many cases can converge faster than GD but it can be very expensive for multidimensional data.  True (Inverting hessian has high computational cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 58\n",
    "\n",
    "Consider document classifier with k classes. Every document is described by at least p\n",
    "ordered words (features). Every word belongs to the dictionary of m values. How many\n",
    "parameters we must estimate for:\n",
    "\n",
    "• Bayes classifier\n",
    "\n",
    "• Naive Bayes classifier\n",
    "\n",
    "• Naive Bayes classifier with the bag-of-model assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can e.g. assume p=100, m =100000, k=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of Naive Bayes classifier all features (words from the dictionary) are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Naive Bayes classifier, for each class we need to calculate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) $$P(x_{1},x_{2},...,x_{m}|Y)=P(x_{1}|Y)P(x_{2}|Y)\\cdot ... \\cdot P(x_{m}|Y),$$\n",
    "\n",
    "where $x_{i}$ denotes words from the dictionary of size $m=100000$.\n",
    "\n",
    "At this point it becomes clear that in this case we need to calculate $k\\cdot m$ parameters $\\left(5\\cdot 10^{6}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we introduce the bag-of-words assumption to our classifier we will only need to perform $k\\cdot p$ comparisons $\\left(5\\cdot 10^{3}\\right)$ as \n",
    "for the m-p words from outside of the analysed document we have the following probability $P(x_{i}=0|Y)=0$. In such a case (1) becomes 0. This is one of the reasons why we should use laplace smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Bayes classifier the order of words matters. Moreover in this case we cannot ignore the dependence between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_{i}|Y) = P(x_{1}|\\wedge_{j\\neq 1} x_{j}Y) P(x_{2}|\\wedge_{j\\neq 2} x_{j}Y)\\cdot ... \\cdot P(x_{m}|\\wedge_{j\\neq m} x_{j}Y)P(\\wedge_{j\\neq m} x_{j}Y),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\wedge_{j\\neq i}$ denotes all possible permutations of dictionary words conjunctions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseqently, for the classifier without the bag-of-words we are looking for $k\\cdot m \\cdot m!$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we introduce the bag of words formalism we will end up with $k\\cdot p \\cdot p!$ parameters to estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Naive Bayes classifier, the Bayes classifier with no 'naive' assumption will not work for $m>p$ unless we employ some smoothing procedure.  That is because for words from the dictionary that cannot be found in the analysed document the conditional probability is equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 59\n",
    "\n",
    "Consider Laplacian smoothing for $k=2$. Discuss cases of:\n",
    "    \n",
    "• $\\alpha = 0.5$\n",
    "\n",
    "• $\\alpha = 1$\n",
    "\n",
    "• $\\alpha = 0$\n",
    "\n",
    "with respect to freqency prediction #$X_{i}$/#$Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe there was a mistake in the formula presented during the lecture. I could probably use some explanation of this problem during the next problem session 5.02!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P\\left(X_{i}|Y=j\\right)=\\frac{\\left[\\#\\text{number of instances such that } X_{i} \\text{ occurs and class label}=j\\right] + \\alpha}{\\left[\\#\\text{number of instances such that class= }j\\right] +\\alpha\\cdot k},$$ where $j=1,...k$; $k$ represents the number of class labels; $\\alpha$ smoothing parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\alpha=0$ we have no smoothing. Consequently if $X_{i}$ doesn't appear in the training set in class $j$ the conditional probability can be written as $P(X_{i}|Y=j)=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\alpha=0.5$, $k=2$ we have:\n",
    "\n",
    "$$P\\left(X_{i}|Y=j\\right)=\\frac{\\left[\\#\\text{number of instances such that } X_{i} \\text{ occurs and class label}=j\\right] + 0.5}{\\left[\\#\\text{number of instances such that class= }j\\right] +1},$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if $X_{i}$ doesn't appear in class $j$ of the training dataset we end up with non-zero $P(X_{i}|Y=j).$ The maximal value of this probability in this case is $\\frac{1}{2}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same goes for $\\alpha=1$ and $k=2$. The maximal value of this probability is also $\\frac{1}{2}$ provided that $X_{i}$ doesn't appear in class $j$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
