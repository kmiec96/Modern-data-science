{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#standard imports \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are three (obligatory) problems to solve due 6.11\n",
    "\n",
    "1.k-NN Problem\n",
    "\n",
    "2.Bayes value Problem\n",
    "\n",
    "3.Expectated value Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The first problem should be solved according to the guidelines from\n",
    "\n",
    "https://github.com/wkrzemien/dataScienceAndML2020/blob/master/notebooks/knn/knn_first.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. KNN algorithm has three steps:\n",
    "\n",
    "(1) Calculate Euclidean Distance.\n",
    "\n",
    "(2) Get Nearest Neighbors.\n",
    "\n",
    "(3)  Make Predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File available here: http://koza.if.uj.edu.pl/~krzemien/machine_learning2021/materials/datasets/iris_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'iris_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'iris_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7143bb3a414f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'iris_data.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6208d269f320>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetosa=data[data['species']=='Iris-setosa']\n",
    "dataVersicolor=data[data['species']=='Iris-versicolor']\n",
    "dataVirginica=data[data['species']=='Iris-virginica']\n",
    "datasets =[dataSetosa,dataVersicolor,dataVirginica]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('species').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('species').std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDatasets(datasets, dataSetsLabels, columnsToPlot=None, xLabel='x', yLabel='y', ):\n",
    "  \"\"\"\n",
    "    Args:\n",
    "      columnsToPlot(list): list of numbers corresponding to columns that should be plotted.\n",
    "                           e.g. if we want to plot 3rd column  vs 4th column columnsToPlot should be\n",
    "                           a list [2,3]. By default it is 1rst vs 2nd so [0,1]\n",
    "\n",
    "  \"\"\"\n",
    "  colors = ['red','green','blue'] \n",
    "  if not columnsToPlot:\n",
    "    columnsToPlot = [0, 1]\n",
    "  for d, dlabel, color in zip(datasets, dataSetsLabels, colors):\n",
    "    xdata = d.iloc[:, columnsToPlot[0]]\n",
    "    ydata = d.iloc[:, columnsToPlot[1]]\n",
    "    plt.scatter(xdata, ydata, label=dlabel, color = color)\n",
    "    plt.xlabel(xLabel)\n",
    "    plt.ylabel(yLabel)\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  # plt.savefig('plot.png') #save figure to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLabels = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDatasets(datasets, dLabels, [0, 1], 'sepal_length', 'sepal_width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDatasets(datasets, dLabels, [2, 3], 'petal_length', 'petal_width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x,y):\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def getNeighbours(x, Xtrain, Ytrain, metric=distance):\n",
    "  \"\"\"\n",
    "    Function should return neighbours of x, sorted by distance from x. \n",
    "    Xtrain and Ytrain form the training set e.g. Xtrain= [[0,1], [10,10]] Y=[0,1]\n",
    "    can be interpreted in the following way: we have two points in the feature space: [0,1] and [10,10]\n",
    "    First point belongs to the class 0 and the second to the class 1.\n",
    "\n",
    "    In order to sort your results by distance one can use sorted construct.\n",
    "    E.g. if we have a list of elements each of elements being the list with 2 elements, then we can sort the main list using e.g. the second element of every object:\n",
    "    testList =[[0,1], [3,0], [2,-1]]   \n",
    "    resultList = sorted(testList, key = itemgetter(1))  # 1 means second element of every \n",
    "    object on the list\n",
    "    We will end up with: [[2,-1], [3,0], [0,1]]\n",
    "\n",
    "    Args: \n",
    "      x(list): list of features(numbers) which represents a point in the feature space with respect to which we calculate the distance.\n",
    "      Xtrain(list): of feature vectors which form our training sample e.g. [[0,1], [10,10]] -> two points [0,1] and [10,10] with two features each\n",
    "      Ytrain(list): of class labels which assing given feature vector (point) to given class e.g. [0,1] the first point belongs to class 0 and the second to 1\n",
    "      metric(function): that calculates distance between two points from the feature space.\n",
    "  \"\"\"\n",
    "  return (-1, [0, 0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getKNNeighbours(neighbours, k):\n",
    "  \"\"\"\n",
    "    Function should return k nearest neighbours assuming that neighbours are already sorted by distance.\n",
    "\n",
    "    Args:\n",
    "      neighbours(list): neighbours sorted by distance e.g. [[1, [2,0],1],  [5, [2,0],1]],\n",
    "                        where [5,[2,0],1] -> distance 5, closest feature point [2,0] which belongs to the class 1\n",
    "      k(integer): number of neighbours to return\n",
    "    Returns:\n",
    "      list of k nearest neighbours assuming that neighbours are already sorted by distance.\n",
    "  \"\"\"\n",
    "  return [-1, [0, 0], 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majorityVote(neighbours):\n",
    "  \"\"\"\n",
    "    Args:\n",
    "      neighbours(list): neighbours sorted by distance e.g. [[1, [2,0],1],  [5, [2,0],1]],\n",
    "                        where [5,[2,0],1] -> distance 5, closest feature point [2,0] which belongs to the class 1\n",
    "    Returns:\n",
    "      number: being the average of class labels  \n",
    "  \"\"\"\n",
    "  return -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, Xtrain, Ytrain, k):\n",
    "  \"\"\"\n",
    "    Args:\n",
    "      x(list): feature point that should be classify\n",
    "      Xtrain(list): of feature vectors which form our training sample e.g. [[0,1], [10,10]] -> two points [0,1] and [10,10] with two features each\n",
    "      Ytrain(list): of class labels which assing given feature vector (point) to given class e.g. [0,1] the first point belongs to class 0 and the second to 1\n",
    "    Returns:\n",
    "     int:  prediction\n",
    "  \"\"\"\n",
    "  return -1\n",
    "\n",
    "\n",
    "def predictList(xObjects, Xtrain, Ytrain, k):\n",
    "  \"\"\"\n",
    "    Args:\n",
    "      xObjects(list): list of feature points that should be classify e.g. [[1,0], [2,2]] -> two points with 2 feature each\n",
    "      Xtrain(list): of feature vectors which form our training sample e.g. [[0,1], [10,10]] -> two points [0,1] and [10,10] with two features each\n",
    "      Ytrain(list): of class labels which assing given feature vector (point) to given class e.g. [0,1] the first point belongs to class 0 and the second to 1\n",
    "    Returns:\n",
    "     list:  list of int corresponding to predictions\n",
    "  \"\"\"\n",
    "  fakePredictions = [0] * len(xObjects)\n",
    "  return fakePredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSquaredError(v, w):\n",
    "  \"\"\"\n",
    "    Args:\n",
    "      v(list): list of numbers\n",
    "      w(list): list of numbers\n",
    "    Returns:\n",
    "     float: mean squared error\n",
    "  \"\"\"\n",
    "  return -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotErrors(x, train,valid):\n",
    "  plt.plot(x, train, label='training error')\n",
    "  plt.plot(x, valid, label='test error')\n",
    "  plt.xlabel('k')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  #plt.savefig('errors.png') #save figure to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divideData(dataSet, fraction):\n",
    "  nbTotal = len(ddataSet.index)\n",
    "  nbTrain = int(nbTotal*fraction)\n",
    "  return (dataSet.iloc[:nbTrain,:], dataSet.iloc[nbTrain:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTests():\n",
    "  # some tests of distance function\n",
    "  np.testing.assert_almost_equal(distance([1, 1], [1, 1]), 0)\n",
    "  np.testing.assert_almost_equal(distance([2, 0, 1], [5, 0, 1]), 3)\n",
    "  np.testing.assert_almost_equal(distance([0, 0], [2, 2]), np.sqrt(8))\n",
    "\n",
    "  # some tests of getNeighbours function\n",
    "  Xtrain = [[2, 0], [0, 0],  [1, 0]]\n",
    "  Ytrain = [1, 0, 1]\n",
    "  x = [-1, 0]  # with respect to x we calculate the distance\n",
    "  result = getNeighbours(x, Xtrain, Ytrain)\n",
    "  expected = ((1, [0, 0], 0), (2, [1, 0], 1), (3, [2, 0], 1))\n",
    "  np.testing.assert_equal(result, expected)\n",
    "\n",
    "  # some tests of getKNNeighbours function\n",
    "  data = [[1, [0, 0], 0], [2, [1, 0], 1], [3, [2, 0], 1]]\n",
    "  np.testing.assert_equal(getKNNeighbours(data,  k=1), [[1, [0, 0], 0]])\n",
    "  np.testing.assert_equal(getKNNeighbours(data,  k=2), [\n",
    "                          [1, [0, 0], 0], [2, [1, 0], 1]])\n",
    "\n",
    "  # some tests of majorityVote function\n",
    "  data = [[1, [0, 0], 0], [2, [1, 0], 1], [3, [2, 0], 1]]\n",
    "  np.testing.assert_equal(majorityVote(data), 2./3.)\n",
    "\n",
    "  # some tests of predict function\n",
    "  xTrain = [[0, 0], [1, 0], [2, 0]]\n",
    "  yTrain = [0, 1, 1]\n",
    "  x = [-1, 0]\n",
    "  k = 1\n",
    "  np.testing.assert_almost_equal(predict(x, xTrain, yTrain, k), 0)\n",
    "  k = 2\n",
    "  np.testing.assert_almost_equal(predict(x, xTrain, yTrain, k), 1)\n",
    "  k = 3\n",
    "  np.testing.assert_almost_equal(predict(x, xTrain, yTrain, k), 1)\n",
    "\n",
    "  # some tests of predictList function\n",
    "  xTrain = [[0, 0], [1, 0], [2, 0]]\n",
    "  yTrain = [0, 1, 1]\n",
    "  xToClassify = [[-1, 0], [3, 0]]\n",
    "  k = 1\n",
    "  np.testing.assert_almost_equal(predictList(xToClassify, xTrain, yTrain, k), [0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runTests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNN(object):\n",
    "    def __init__(self, n_neighbours):\n",
    "        self.n_neighbours = n_neighbours\n",
    "        self.Xtrain = []\n",
    "        self.Ytrain = []\n",
    "    def fit(self, x,y):\n",
    "        self.Xtrain = x\n",
    "        self.Ytrain = y\n",
    "    def predict(self, Xobjects):\n",
    "        return predictList(Xobjects,self.Xtrain, self.Ytrain, self.n_neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = [[0, 0], [1, 0], [2, 0]]\n",
    "yTrain = [0, 1, 1]\n",
    "xToClassify = [[-1, 0], [3, 0]]\n",
    "k = 1\n",
    "model = kNN(k)\n",
    "model.fit(xTrain,yTrain)\n",
    "np.testing.assert_almost_equal(model.predict(xToClassify), [0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classLabels = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "columnLabels = ['sepal_length', 'sepal_width',\n",
    "                  'petal_length', 'petal_width', 'species']\n",
    "# First let's try  Iris-setosa vs Iris-versicolor, two features sepal-length vs sepal-width\n",
    "# we get rid of the third class Iris-viriginica\n",
    "dataWithoutVirignica = data[data['species'] != 'Iris-virginica']\n",
    "\n",
    "def toNumeric(row):\n",
    "    if row.species == 'Iris-versicolor':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "dataWithoutVirignica['class']=dataWithoutVirignica.apply(lambda row:toNumeric(row),axis=1)\n",
    "  \n",
    "# we leave only sepal_length, sepal_width and class label columns\n",
    "columnsToLeave=['sepal_length', 'sepal_width', 'class']  \n",
    "dataSepal = dataWithoutVirignica[columnsToLeave]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 0.8\n",
    "# we  divide the content into the training and validation set\n",
    "# we also randomly shuffle the content\n",
    "trainingSet=dataSepal.sample(frac=fraction)\n",
    "validationSet=dataSepal.drop(trainingSet.index)\n",
    "xTrain = trainingSet.iloc[:, 1:].values.tolist()\n",
    "yTrain = trainingSet.iloc[:, 0].values.tolist()\n",
    "xValid = validationSet.iloc[:, 1:].values.tolist()\n",
    "yValid = validationSet.iloc[:, 0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fraction < 1:\n",
    "    #we calculate training error and validation error\n",
    "    trainError=[]\n",
    "    validError=[]\n",
    "    kRange =range(1,80)\n",
    "    trainPredictions = predictList(xTrain, xTrain, yTrain, 1)\n",
    "    for k in kRange:\n",
    "      trainPredictions = predictList(xTrain, xTrain, yTrain, k)\n",
    "      validationPredictions = predictList(xValid, xTrain, yTrain, k)\n",
    "      trainError.append(meanSquaredError(trainPredictions,yTrain))\n",
    "      validError.append(meanSquaredError(validationPredictions,yValid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotErrors(kRange, trainError,validError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bayes value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pharmaceutical company developed a test for detecting the rare disease, which is carried by 0.5 % cases of the whole population. Letâ€™s assume that the test gives positive results for 96% of the cases if the patient is ill, but it also gives positive results in 5% of the healthy patient. What is the probability that a patient is ill if his test gave a positive result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The natural solution to this problem is (almost the same as https://www.youtube.com/watch?v=R13BD8qKeTg):\n",
    "    \n",
    "$$P(\\text{ill}|P)=\\frac{P(P|\\text{ill})\\cdot P(\\text{ill})}{P(ill) \\cdot P(P|ill)+P(-ill) \\cdot P(P|-ill)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(\\text{ill})=0.005$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(-\\text{ill})=0.995$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(P|\\text{ill})=TP=0.960$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(P|\\text{-ill})=FP=0.050$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that the patient is ill on condition that they were positively diagnosed is: 8.799%.\n"
     ]
    }
   ],
   "source": [
    "#Calculations\n",
    "P_ill=0.005\n",
    "P_not_ill=0.995\n",
    "P_P_ill=0.960\n",
    "P_P_not_ill=0.050\n",
    "\n",
    "P_ill_P=((P_P_ill*P_ill)/(P_ill*P_P_ill+P_not_ill*P_P_not_ill))\n",
    "\n",
    "print(\"The probability that the patient is ill on condition that they were positively diagnosed is: {:.4g}%.\".format(100*P_ill_P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Expected Value Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a).Let $g(X) = 1$ for some set $A$ being a subset of sample space $\\Omega$. What is the interpretation of $E[g(X)]$ if is discrete with a given PMF or continuous with a given PDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve this problem we have to make an assumption about the values of $g(x)$ for $x\\notin A$. Since the author of this question did not specify what happens for x from within $\\Omega$ but outside of $A$ region we have taken the liberty to specify function $g(x)$ just a tap more\n",
    "\n",
    "$$g(x)= \n",
    "     \\begin{cases}\n",
    "      &1, x\\in A\\\\\n",
    "      &0, x\\notin A\n",
    "     \\end{cases}.$$\n",
    "     \n",
    "For continuous $x \\in A$:\n",
    "$$E\\left[g(X)\\right]=\\int_{\\Omega}g(x)\\cdot f_{x}(x) dx=\\int_{\\Omega} f_{x}(x) dx = 1,$$\n",
    "\n",
    "where in accordance to the lecture $f_{x}(x)$ is PDF function.  \n",
    "\n",
    "For discrete $k\\in A$ and function $g(x)$ defined in the aforementioned way it is only natural to conclude that:\n",
    "\n",
    "$$E\\left[g(X)\\right]=\\int_{\\Omega}g(x)\\cdot f_{x}(x) \\cdot \\delta(x-k) dx=\\sum_{k\\in A}g(k)\\cdot p_{x}(k)=\\sum_{k\\in A}p_{x}(k)=1,$$\n",
    "\n",
    "where $p_{x}(k)$ is the PMF function.\n",
    "\n",
    "The above equation can be trivially generalised to $n$ dimensions: $$x\\rightarrow (x_{1},x_{2},.....,x_{n}),$$ $$dx\\rightarrow dx_{1}dx_{2}...dx_{n}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b).What is the interpretation of $E\\left[g(X)\\right]$ for $g(X)=x$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E\\left[g(X)\\right]=\\int_{\\Omega}g(x)\\cdot f_{x}(x) dx=\\int_{\\Omega}x\\cdot f_{x}(x) dx.$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
