{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions to Modern Data Science Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# probelms till 15.02 + 20.02 + problem 33 (Optional)\n",
    "\n",
    "by *Mateusz Kmieć* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 33. (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gauss-Markov theorem\n",
    "\n",
    "Show that the ordinary least squares estimator has the lowest variance among all\n",
    "unbiased linear estimators (is an “efficient” estimator) under the assumptions that:\n",
    "\n",
    "- Its errors have zero expectation values\n",
    "- its variances are equal.\n",
    "- errors are uncorrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 51. (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the gradient descent algorithm for linear regression:\n",
    "start with some initial $\\left(\\theta_{1}, \\theta_{0} \\right)$ for every iteration:\n",
    "\n",
    "$$\\theta_{k+1} = \\theta_{k} - \\alpha \\cdot \\frac{\\partial E(\\theta_{k})}{\\partial \\theta}.$$,\n",
    "\n",
    "where $k=0,1$, $\\alpha$ – learning rate\n",
    "\n",
    "Test your implementation with some data (e.g. x03.csv) and compare with analytical\n",
    "solution:\n",
    "\n",
    "- plot $E(\\theta_{k})$ vs iteration\n",
    "- plot $\\theta_{k}$ vs iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>inhabitants</th>\n",
       "      <th>incomeBelow5k</th>\n",
       "      <th>unemployed</th>\n",
       "      <th>murders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>587000</td>\n",
       "      <td>16.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>643000</td>\n",
       "      <td>20.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>635000</td>\n",
       "      <td>26.3</td>\n",
       "      <td>9.3</td>\n",
       "      <td>40.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>692000</td>\n",
       "      <td>16.5</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1248000</td>\n",
       "      <td>19.2</td>\n",
       "      <td>7.3</td>\n",
       "      <td>24.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  inhabitants  incomeBelow5k  unemployed  murders\n",
       "0      1       587000           16.5         6.2     11.2\n",
       "1      2       643000           20.5         6.4     13.4\n",
       "2      3       635000           26.3         9.3     40.7\n",
       "3      4       692000           16.5         5.3      5.3\n",
       "4      5      1248000           19.2         7.3     24.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Please note 'raw' prefix in the url and the lack of 'blob' part\n",
    "url='https://raw.github.com/wkrzemien/dataScienceAndML2020/master/datasets/x08.csv'\n",
    "urllib.request.urlretrieve(url,'x08.csv')\n",
    "\n",
    "FILE_NAME = 'x08.csv'\n",
    "\n",
    "data = pd.read_csv(FILE_NAME, names=['index','inhabitants','incomeBelow5k','unemployed','murders'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filename):\n",
    "    columns = ['index','inhabitants','incomeBelow5k','unemployed','murders']\n",
    "    data = pd.read_csv(filename, names=columns)\n",
    "    \n",
    "    fraction = 0.75\n",
    "    trainingSet=data.sample(frac=fraction)\n",
    "    testSet=data.drop(trainingSet.index)\n",
    "    \n",
    "    dataset1 = trainingSet['inhabitants']\n",
    "    dataset2 = trainingSet['incomeBelow5k']\n",
    "    dataset3 = trainingSet['unemployed']\n",
    "    dataset4 = trainingSet['murders']\n",
    "    \n",
    "    testset1 = testSet['inhabitants']\n",
    "    testset2 = testSet['incomeBelow5k']\n",
    "    testset3 = testSet['unemployed']\n",
    "    testset4 = testSet['murders']\n",
    "    \n",
    "    \n",
    "    return columns[1:4], [dataset1, dataset2, dataset3], dataset4, [testset1, testset2, testset3], testset4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySimpleLinearRegression():\n",
    "    def __init__(self):\n",
    "        self._theta = [0, 0]\n",
    "        self._error = 0\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        size=len(X)\n",
    "        xx=np.array(X).reshape(-1, 1).reshape(size,1)\n",
    "        x = np.asmatrix(np.c_[np.ones((size,1)),xx])\n",
    "        y = np.array(Y).reshape(-1, 1).reshape(size,1)  \n",
    "        \n",
    "        #calculation of parameters, closed form solution, linear regression OLS.\n",
    "        theta = np.linalg.inv(x.T*x)*x.T*y    \n",
    "        self._theta = [theta.item(0),theta.item(1)] \n",
    "        \n",
    "        return  self._theta\n",
    "\n",
    "    def getParams(self):\n",
    "        return self._theta\n",
    "\n",
    "    def predict(self, x):         \n",
    "        return self._theta[1]*x+self._theta[0] \n",
    "    \n",
    "    def getError(self, X, Y):\n",
    "        size=len(X)\n",
    "        xx=np.array(X).reshape(-1, 1).reshape(size,1)\n",
    "        x = np.asmatrix(np.c_[np.ones((size,1)),xx])\n",
    "        y = np.array(Y).reshape(-1, 1).reshape(size,1)  \n",
    "        theta=np.matrix([[self._theta[0]],[self._theta[1]]])        \n",
    "        error=(((x@theta-y).T)@(x@theta-y))*(1/y.shape[0])\n",
    "        self._error = error[0,0]\n",
    "        return self._error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(data1, data2, test1, test2, names):\n",
    "    \n",
    "    #model fitting\n",
    "    model = MySimpleLinearRegression()\n",
    "    model.fit(data1, data2)\n",
    "    print(\"Parameters [intercept, slope] = \",model.getParams(),\",\")\n",
    "    yPredicted = [model.predict(x) for x in data1]\n",
    "        \n",
    "    #Training error\n",
    "    print(\"Training error = {:.4f}\".format(model.getError(data1,data2)))\n",
    "      \n",
    "    #Test error\n",
    "    yPredicted_test = [model.predict(x) for x in test1]\n",
    "    print(\"Test error = {:.4f}\".format(model.getError(test1,test2)))\n",
    "    \n",
    "    #plot the best line and data\n",
    "    plt.scatter(data1, data2)\n",
    "    plt.plot(data1, yPredicted)  \n",
    "    plt.xlabel(names)\n",
    "    plt.ylabel('murder')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData_sklearn(data1, data2, test1, test2, names):\n",
    "    \n",
    "    #formatting data\n",
    "    data1 = np.array(data1).reshape(-1, 1).reshape(len(data1),1)\n",
    "    data2 = np.array(data2).reshape(-1, 1).reshape(len(data2),1)   \n",
    "    test1 = np.array(test1).reshape(-1, 1).reshape(len(test1),1)\n",
    "    test2 = np.array(test2).reshape(-1, 1).reshape(len(test2),1)\n",
    "  \n",
    "    #model fitting\n",
    "    model = LinearRegression().fit(np.array(data1), np.array(data2))\n",
    "\n",
    "    print(\"Parameters [intercept, slope] = \",\"[\",model.intercept_[0],model.coef_[0,0],\"],\")\n",
    "    ypred = model.predict(data1)\n",
    "        \n",
    "    #Training error\n",
    "    print(\"Training error = {:.4f}\".format(mean_squared_error(data2, ypred)))\n",
    "      \n",
    "    #Test error\n",
    "    ytest = model.predict(np.array(test1))\n",
    "    print(\"Test error = {:.4f}\".format(mean_squared_error(test2,ytest)))\n",
    "    \n",
    "    #plot the best line and data\n",
    "    plt.scatter(data1, data2)\n",
    "    plt.plot(data1, ypred)  \n",
    "    plt.xlabel(names)\n",
    "    plt.ylabel('murder')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESULTS FOR MySimpleLinearRegression()\")\n",
    "print(\"--------------------------------------\\n\")\n",
    "\n",
    "names, datas, murder, test_sets, test_murder = loadData(FILE_NAME)\n",
    "for i,d in enumerate(datas):\n",
    "    plotData(d.tolist(), murder.tolist(), test_sets[i].tolist(), test_murder.tolist(), names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESULTS FOR LinearRegression() from sklearn\")\n",
    "print(\"--------------------------------------\\n\")\n",
    "\n",
    "#names, datas, murder, test_sets, test_murder = loadData(FILE_NAME)\n",
    "for i,d in enumerate(datas):\n",
    "    plotData_sklearn(d.tolist(), murder.tolist(), test_sets[i].tolist(), test_murder.tolist(), names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, m_current=0, b_current=0, epochs=1000, learning_rate=0.0001):\n",
    "    N = float(len(y))\n",
    "    for i in range(epochs):\n",
    "        y_current = (m_current * X) + b_current\n",
    "        cost = sum([data**2 for data in (y-y_current)]) / N\n",
    "        m_gradient = -(2/N) * sum(X * (y - y_current))\n",
    "        b_gradient = -(2/N) * sum(y - y_current)\n",
    "        m_current = m_current - (learning_rate * m_gradient)\n",
    "        b_current = b_current - (learning_rate * b_gradient)\n",
    "    return m_current, b_current, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I implemented LEAVE ONE OUT algorithm\n",
    "x = np.array([188, 181, 197, 168, 167, 187, 178, 194, 140, 176, 168, 192, 173, 142, 176]).reshape(-1, 1).reshape(15,1)\n",
    "y = np.array([141, 106, 149, 59, 79, 136, 65, 136, 52, 87, 115, 140, 82, 69, 121]).reshape(-1, 1).reshape(15,1)\n",
    "\n",
    "x = np.asmatrix(np.c_[np.ones((15,1)),x])\n",
    "\n",
    "I = np.identity(2)\n",
    "alpha_list = np.linspace(start = 0.0, stop = 0.1, num = 1000)# change here\n",
    "costlist=[]\n",
    "sorted_cost=[]\n",
    "\n",
    "# add 1-3 line of code here\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    error=0\n",
    "    for k in range(y.shape[0]):\n",
    "        xk=x\n",
    "        yk=y\n",
    "        xk=np.delete(xk, k, axis=0)\n",
    "        yk=np.delete(yk, k, axis=0)\n",
    "        wk = np.linalg.inv(xk.T@xk + alpha * I)@xk.T@yk\n",
    "        wk=wk.ravel()\n",
    "        error+=((y-x@wk.T)[k]*(y-x@wk.T)[k])/(y.shape[0])\n",
    "        #czy tu powinno być cost=RSS+alpha*w*w.T?\n",
    "    \n",
    "    costlist.append(error[0,0])\n",
    "\n",
    "min_index = costlist.index(min(costlist))   \n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.plot(alpha_list, costlist, color='blue', linewidth = 1, zorder=1)\n",
    "plt.scatter(alpha_list[min_index],costlist[min_index], color='red', zorder=2)\n",
    "plt.title('CROSS-VALIDATION')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('error')\n",
    "plt.show()\n",
    "\n",
    "# add 1-3 lines to compare the results\n",
    "print(\"I picked 1000 alphas from within (0,0.1).\")\n",
    "print(\"Alpha which leads to the lowest cost is {0:.6f}\".format(alpha_list[min_index]))\n",
    "\n",
    "print(\"\\nDONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_Newton(f, dfdx, x, eps):\n",
    "    while abs(f(x)) > eps:\n",
    "        x = x - float(f(x))/dfdx(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2 - 9\n",
    "\n",
    "def dfdx(x):\n",
    "    return 2*x\n",
    "\n",
    "print naive_Newton(f, dfdx, 1000, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probelm 52. (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Newton method and test your implementation on data from x03.csv. Once you do it you shall compare it with the analytical\n",
    "solution:\n",
    "\n",
    "- plot $E(\\theta_{k})$ vs iteration\n",
    "\n",
    "- plot $\\theta_{k}$ vs iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 60. (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your version of Naive Bayes classifier that reads the training\n",
    "sample in csv format. For simplicity you can assume that the input contains only two\n",
    "classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 61."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the 1-D box kernel with the width w=2 and the following set of data \n",
    "$x_{i} = [1,2.5, 3,3.5,4,6,6.5,7,9]$, use KDE method to estimate pdf for points:\n",
    "    \n",
    "- f(0.5)\n",
    "- f(4)\n",
    "- f(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 62. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program that implements 1-D KDE estimation. Plot the estimated pdf for the\n",
    "box kernel and the sample data. Plot the same pdf using the gaussian kernel instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probelm 63."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a program to calculate the integral of $f(x) =cos(x)$ and $x$ in $\\left[0,\\frac{\\pi}{2}\\right]$ using MC\n",
    "integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probelm 64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that MC integral error decreases as $\\frac{1}{\\sqrt{N}}$ . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will present the heuristic proof of this property MC integrating algorithms based on importance sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central equation of importance sampling in 1-D is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) $$\\int_{a}^{b}f(x)dx \\cong \\frac{(b-a)}{N}\\sum_{i=1}^{N} f(x_{i}) \\cong (b-a)E[f(x)].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us notice that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) $$\\text{Var}(\\text{mean})=\\text{Var}\\left(\\frac{1}{N}\\sum_{1}^{N}X_{i}\\right)=\\frac{1}{N^{2}}\\text{Var}\\left(\\sum_{i=1}^{N}X_{i}\\right)=\\frac{1}{N^{2}}\\text{Var}(X_{1}+...+X_{N})=\\frac{N}{N^{2}}\\text{Var(X)}=\\frac{1}{N}\\text{Var}\\left(X\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, (1) can be written as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) $$\\text{Var}\\left((b-a)E\\left[f(x)\\right]\\right)=\\frac{(b-a)^{2}}{N}\\text{Var}\\left(f(x)\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the integration error can be written as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) $$\\text{error}=\\sqrt{\\text{Var}\\left((b-a)E(f(x))\\right)}=\\frac{(b-a)}{\\sqrt{N}}\\sqrt{\\text{Var}(f(x))}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which can be generelised for $n$-dimentsions (integration over n-dimensional space of volume $V$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) $$\\text{error}=\\frac{V}{\\sqrt{N}}\\sqrt{\\text{Var}(f(x))}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the form of equetion (5) is independent of the number of dimensions that we are integrating over. The aforementioned property makes the MC integration insensitive to the curse of dimensionality QED."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 65."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a random generator using rejection sampling method for $f(x) = sin (x)$ and x in $\\left[0,\\frac{\\pi}{2}\\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 66."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a random generator that generates exponential distribution using inverse\n",
    "sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 67."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a generator that simulates the decay of a particle with mass M into\n",
    "two photons. Let’s assume that the particle is moving in the LAB frame with some velocity\n",
    "V. Plot the simulated angular distributions in the LAB and CM frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probelm 68. (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give examples of the NN weights values that would act as OR and NOT gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 69."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate derivatives of the activation functions and express it (if possible) by the\n",
    "function itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On slide 44 lecture 12 the activation function was chosen to be the logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(44) $$\\frac{d}{d z}S(z)=\\frac{d}{d z}\\left(\\frac{1}{1+e^{-z}}\\right)=\\frac{e^{-z}}{(1+e^{-z})^{2}}=\\frac{1}{1+e^{-z}}\\frac{e^{-z}}{1+e^{-z}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first factorial in (44) is S(x) therefore:\n",
    "\n",
    "(45)$$\\frac{d}{d z}S(z)=S(z)\\frac{-1+1+e^{-z}}{1+e^{-z}}=S(z)(1-S(z)).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next prominent activation function mentioned during the lecture was $\\text{RelU}(0,z) = \\text{max}(0,z)$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily see that in this case the derivative is the following piecewise function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d}{dz}\\text{RelU}(z,0)=\\begin{cases} \n",
    "      1 & z \\geq 0 \\\\\n",
    "      0 & z < 0.\n",
    "   \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivallent to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d}{dz}\\text{RelU}(z,0)=\\begin{cases} \n",
    "      \\text{RelU}(1,0) & z \\geq 0 \\\\\n",
    "      \\text{RelU}(-1,0) & z < 0.\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lecturer presented $\\text{tanh}(z)$ activation function. In this case we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{tanh}(z)=\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d}{dz}\\text{tanh}(z)=\\frac{\\left(e^{z}+e^{-z}\\right)\\left(e^{z}+e^{-z}\\right)-\\left(e^{z}-e^{-z}\\right)\\left(e^{z}-e^{-z}\\right)}{\\left(e^{z}+e^{-z}\\right)^{2}}=1-\\text{tanh}^{2}(z).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last activation function mentioned during the lecture is the softplus function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(z)=\\ln(1+\\exp(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d}{dz}f(z)=\\frac{e^{z}}{1+e^{z}}=\\frac{1}{1+e^{-z}}=\\text{logistic function}=S(z).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 70."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the backpropagation for logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
