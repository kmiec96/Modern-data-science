{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#standard imports \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 35\n",
    "We consider two models for classification: (1) “flexible” (e.g. kNN) (2) “rigid ” (e.g. linear). Discuss which would you typically choose in those situations:\n",
    "* A small number of training samples N\n",
    "* A large number of features\n",
    "* A large number of training samples N\n",
    "* Highly non-linear behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 37\n",
    "We want to check if the coin is fair, we obtained the following result: X = {H,T,H,H,T,T,H,H,H}.\n",
    "* define the PMF\n",
    "* define the likelihood function\n",
    "* calculate the maximum value estimator of frequency of heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 38\n",
    "When MAP and MLE give the same results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 39\n",
    "We want to check if the coin is fair, we estimate the frequency of heads. Calculate the MAP maximum value estimator, when the prior distribution is described by the beta function\n",
    "$$\\text{pdf}(p|\\alpha, \\beta) = (\\alpha-1)! * ( \\beta-1)!/(\\alpha+ \\beta -1)! * p\\alpha-1 * (1-p)\\beta-1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40\n",
    "Consider the case of $\\lambda >> 1$, and $\\lambda \\approx 0$: how does it affect the overfitting/bias problem? Will the procedure work correctly anyway?\n",
    "\n",
    "$$ E_T[L(f_\\theta(x),Y)]=\\frac{1}{2N} \\sum (\\theta^T x^{(i)}-y^{(i)})^2 + \\frac{\\lambda}{2}\\sum \\theta_j^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 41\n",
    "Read the code and try to understand how it works. By changing the values of degree and alpha observe the interplay of overfitting/bias\n",
    "* for which values one can obtained the reasonable fit to data ?\n",
    "* plot the training and test errors as a function of polynomial degree (for fixed alpha)\n",
    "* plot the training and test errors as a function of regularization term (for fixed degree). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 42\n",
    "We consider a linear regression model. Let’s assume that the error model is additive $y= f(x) + \\epsilon$ , and errors are have Gaussian distribution with 0 mean ($E[\\epsilon] =0$) and variance\n",
    "($\\text{Var}[\\epsilon] =\\sigma^2$). We In addition, assume that our parameters are described by:\n",
    "* Gaussian Distribution, with 0 mean, and some variance\n",
    "* Laplace Distribution, with 0 mean, and some variance\n",
    "Derive the minimization problem starting from the MAP approach, taking into account the prior distribution of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 43 \n",
    "We consider a classification model with some regularization scheme. Which statements\n",
    "are true? Justify your choice.\n",
    "* Adding regularization would typically lower the training error\n",
    "* Adding new features to your model would make the training error higher or the same\n",
    "* Adding regularization would typically lower the test error\n",
    "* Adding regularization can cause overfitting of your model\n",
    "* Using a large value of the lambda parameter can cause the overfitting of your model\n",
    "* Adding regularization can introduce the bias to your model\n",
    "* Adding regularization can make the training error higher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 44\n",
    "Calculate the derivative of the sigmoid function and express it using the sigmoid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 45 \n",
    "Assuming the loss function definition as:\n",
    "$$ L(f_\\theta(x),Y)= -log f_\\theta(x) \\text{ for $Y=1$}\\\\\n",
    "                   = -log (1-f_\\theta(x)) \\text{ for $Y=0$}$$\n",
    "what are the loss values for:\n",
    "* $f_\\theta(x)=0$, Y=0\n",
    "* $f_\\theta(x)=1$, Y=0\n",
    "* $f_\\theta(x)=0$, Y=1\n",
    "* $f_\\theta(x)=1$, Y=1\n",
    "\n",
    "Draw L as a function of $f_\\theta(x)$ for Y= 0 and Y=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 46\n",
    "Calculate the equation of the decision boundary for the logistic regression model assuming:\n",
    "\n",
    "$$ f_\\theta(x)\\geq 0.5 \\to Y=1\\\\\n",
    "f_\\theta(x)< 0.5 \\to Y=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 47\n",
    "* For which value of K K-fold cross-validation is equivalent to Leave-one-out method?\n",
    "* Order the cross-validation methods (K-fold e.g. K=5, leave-one-out and the simple splitting into validation and training sets) in the ascending cost of the computation power\n",
    "* For which K the K-fold cross-validation requires the most of the computing power?\n",
    "\n",
    "Justify your answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 48\n",
    "We consider two cases of binary classifier tests:\n",
    "* A - airplanes security checks before the take-off (Y=’allowed’/’denied’)\n",
    "* B - decision if according to her/his profile a client will be potentially interested to buy a new product and if to show her/him the targeted advertisement(Y=’yes’/’no’)\n",
    "\n",
    "How would we decide how to tune our classifier threshold for each case in the context of sensitivity vs specificity trade-off? Justify your answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
